import os
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor

# --- 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥ ---
print("="*50)
print("[Step 1] Checking Hardware & Drivers...")
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
device_name = torch.cuda.get_device_name(0)
print(f"GPU Device: {device_name}")

# æ£€æŸ¥ Flash Attention æ˜¯å¦å¯ç”¨ (A800 å¿…å¤‡)
try:
    # å°è¯•å¯¼å…¥ flash_attn åŒ…
    import flash_attn
    print("Flash Attention 2 Package: INSTALLED âœ…")
    use_flash = True
except ImportError:
    print("Flash Attention 2 Package: NOT FOUND âŒ (Running slow mode)")
    use_flash = False

# --- 2. ä»¿çœŸç¯å¢ƒæ£€æŸ¥ ---
print("\n[Step 2] Checking Simulation Environment...")
# å¼ºåˆ¶ä½¿ç”¨ EGL åç«¯ (å› ä¸ºæœåŠ¡å™¨æ²¡æœ‰æ˜¾ç¤ºå™¨)
os.environ['MUJOCO_GL'] = 'egl' 
try:
    import mujoco
    import libero.libero
    print("MuJoCo & LIBERO: IMPORT SUCCESS âœ…")
except Exception as e:
    print(f"LIBERO Error: {e} âŒ")

# --- 3. æ¨¡å‹åŠ è½½æµ‹è¯• ---
print("\n[Step 3] Loading OpenVLA-7B (Downloading ~15GB)...")
print("âš ï¸  This may take 5-10 minutes. Please wait...")

model_id = "openvla/openvla-7b"

try:
    # åŠ è½½å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    
    # åŠ è½½æ¨¡å‹ (è‡ªåŠ¨ä½¿ç”¨ Flash Attention 2)
    vla = AutoModelForVision2Seq.from_pretrained(
        model_id, 
        attn_implementation="flash_attention_2" if use_flash else "eager",
        torch_dtype=torch.bfloat16, 
        low_cpu_mem_usage=True, 
        trust_remote_code=True
    ).to("cuda:0")
    
    print("\nğŸ‰ğŸ‰ğŸ‰ SUCCESS! OpenVLA loaded on " + device_name)
    print("Environment is 100% READY for experiments.")

except Exception as e:
    print(f"\nâŒ Model Load Failed: {e}")
    print("Possible reasons: HF Token not valid, Network issue, or Flash Attn mismatch.")

print("="*50)
